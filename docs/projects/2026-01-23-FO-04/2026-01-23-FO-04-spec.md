# FO-04 Specification

**Created:** 2026-01-23
**Status:** Ready for Implementation

## Overview

FO-04 builds on FO-03 with one key change: the detail-gathering screens between "what do you want affirmations to help with" and the affirmation generation are now dynamically generated by the AI agent instead of being hard-coded.

### Flow Summary

1. **Initial screens (identical to FO-03):** All screens up to and including "what do you want affirmations to help with"
2. **Dynamic detail-gathering screens (NEW):** 2-5 AI-generated screens that progressively gather context
3. **Affirmation generation (same as FO-03):** Generate and display affirmations

## Purpose & Philosophy

### Why Dynamic Onboarding?

The onboarding is not just data collection — it is part of the emotional experience. The questions themselves should make the user feel seen and understood. By generating questions dynamically, the agent can:
- Adapt to what the user has already shared
- Avoid asking irrelevant or repetitive questions
- Create a warm, conversational flow that feels personal

### What Makes Affirmations Work

Affirmations fail when they:
- Are too far removed from the user's lived reality
- Feel like something *others* are saying, not something *I* can say
- Create resistance ("that doesn't feel true")

Affirmations succeed when they:
- Sit just one step ahead of the user's current inner state
- Match the user's inner language
- Reduce inner friction instead of creating it

A successful affirmation should feel like: *"This understands me — and I can actually say this to myself."*

### What the Agent Needs to Understand

Before generating affirmations, the agent should gather enough to understand:

| Dimension | Why It Matters |
|-----------|----------------|
| **Emotional baseline** | The user's current emotional state determines tone and intensity. Affirmations that are too upbeat feel fake; too neutral feel empty. |
| **Inner dialogue** | How the user talks to herself. Affirmations should be emotionally digestible counter-phrases to the inner critic. Generic positivity triggers "that's not me." |
| **Needs & longings** | What she wants more of or less of. Relevance creates impact — affirmations should support what the user lacks and soothe what weighs on her. |
| **Believability threshold** | What she can emotionally accept today. Exaggerated phrases trigger resistance. Match what she can realistically say to herself, especially on difficult days (e.g., "I am…", "I'm learning to…", "I allow myself to…"). |
| **Life context (light touch)** | Where this shows up in her life. Gentle context increases recognition and personal relevance, without becoming heavy or therapeutic. |

### Prior Context Available to Agent

Before the dynamic screens begin, the agent already knows:
- The user's **name**
- Whether they are **new to affirmations or experienced**
- What they **hope to get out of affirmations** (from the initial topic selection)

This context should shape tone and questions, not be repeated mechanically.

## Dynamic Detail-Gathering Screens

### Screen Structure

Each dynamic screen contains (from top to bottom):

1. **Reflective statement** - One sentence summarizing what we've learned so far about the user *(omitted on first screen)*
2. **Question** - One sentence asking for more detail (same styling as reflective statement, with spacing between them)
3. **Input field** - Free-text input with inline chip support
4. **Suggestion chips** - AI-generated options with `+` prefix (e.g., "+ work stress")
5. **Expand option** - "Show more" to reveal additional chips

**First screen special case:**
- No reflective statement (nothing to reflect on yet)
- Question: "What has been going on lately that brought you here?"
- Chips are AI-generated based on the initial topic

**Visual notes:**
- Reflective statement and question have equal weight/styling
- Visual spacing separates them so they read as two distinct elements (statement + question)
- No progress indicator (step count is dynamic, would confuse users)

**Loading state:**
- While waiting for agent to generate the next screen, show "Thinking..."

**Error handling:**
- If agent call fails, show error message and let user retry manually

### Chip Interaction UX

- Chips displayed below input field have a `+` prefix to indicate they can be added
- Clicking a chip **inserts it as a small tag inside the input field** (not colored/highlighted in place)
- Chips inside the input field have a `-` or `x` to remove them
- User can combine: type freely AND add chips
- Everything in the input field (typed text + chip tags) becomes the combined answer
- **Validation:** "Next" button is disabled until there's either text typed or at least one chip selected

### Agent Behavior

**Input to agent:** Full history of all previous questions and user answers (selected chips + typed text)

**Output from agent:**
- Reflective statement *(empty string on first screen)*
- New question
- Initial chips: 5-8 chips (shown by default)
- Expanded chips: 10-15 chips (shown on "show more")
- Confidence signal: ready to proceed to affirmations? (boolean)

**Termination rules:**
- Minimum: 2 screens
- Maximum: 5 screens
- Within range: Agent decides when confident enough information has been gathered
- No user "skip" option - agent controls the flow

### Tone & Voice Guidelines

The agent's reflective statements, questions, and chip suggestions must feel:
- Warm, calm, and respectful
- Non-clinical and non-judgmental
- Supportive, not instructive
- Simple and human

**Avoid:**
- Therapy or diagnostic language
- Spiritual clichés
- Exaggerated positivity
- Long explanations

**Aim for:**
- Short, easy-to-answer questions
- Everyday language
- A sense of "we're doing this together"
- Emotional safety and trust

### Question Design Principles

All questions should:
- Feel optional and pressure-free
- Be answerable with short text or simple chip selections
- Feel like invitations, not tests

Good questions:
- Start soft and neutral
- Normalize common feelings
- Offer examples or options (via chips) when helpful
- Never assume something is "wrong"

**Suggested progression across screens:**
1. How things feel right now
2. What the user tends to struggle with
3. What she wishes she could feel instead
4. What kind of support feels right
5. What an ideal affirmation should help her remember

## Technical Considerations

### Agent API Shape

The agent should return a structured response:

```typescript
interface DynamicScreenResponse {
  reflectiveStatement: string;   // Empty string on first screen
  question: string;
  initialChips: string[];        // 5-8 chips, shown by default
  expandedChips: string[];       // 10-15 chips, shown on "show more"
  readyForAffirmations: boolean;
}
```

### State Management

Each agent call receives the accumulated context:

```typescript
interface GatheringContext {
  // User profile (from initial screens)
  name: string;
  familiarity: 'new' | 'some' | 'very';  // Experience with affirmations
  initialTopic: string;                   // From "what do you want affirmations to help with"

  // Conversation history
  exchanges: Array<{
    question: string;
    answer: {
      text: string;            // Free-typed text
      selectedChips: string[]; // Chips added to input
    };
  }>;
  screenNumber: number;        // 1-indexed, used for min/max enforcement
}
```

### Screen Count Logic

```
if screenNumber < 2:
  ignore readyForAffirmations, show next screen
else if screenNumber >= 5:
  proceed to affirmations regardless
else:
  respect agent's readyForAffirmations decision
```

## Desired Outcome

Based on the gathered information, the generated affirmations should:
- Match how the user feels right now (tone and intensity)
- Sound natural in the user's inner voice
- Support what the user lacks
- Soothe what currently weighs on her
- Feel emotionally safe and believable
- Feel relevant to her life right now
- Match her experience level (new vs experienced)
- Guide forward gently, without pressure

## Testing Strategy

### E2E Testing (Required)

Before FO-04 is considered complete, E2E tests must be written and passing using the existing Playwright infrastructure.

**Test file:** `e2e/fo-04.test.ts`

**Test coverage must include:**
1. Full navigation through initial screens (identical to FO-03)
2. Dynamic detail-gathering flow:
   - Verify reflective statement and question appear
   - Test chip selection (clicking adds chip to input field)
   - Test chip removal (clicking `-` removes chip)
   - Test free-text input
   - Test combined input (text + chips)
   - Verify "Next" is disabled until input provided
   - Test "Show more" chip expansion
3. Verify 2-5 screen flow (agent controls progression)
4. Transition to affirmation generation
5. Affirmation display and swipe phase (same as FO-03)

**Running tests:**
```bash
npm run dev  # Start dev server in one terminal
node --import tsx e2e/fo-04.test.ts  # Run test in another
```

See `docs/current/e2e-testing.md` for full testing documentation.

### Manual Testing

- Verify tone and quality of AI-generated reflective statements, questions, and chips
- Confirm the flow feels warm, conversational, and not repetitive
- Test edge cases: very short answers, very long answers, only chips, only text

## Open Questions

None - all resolved.

## References

- Design brief: `docs/projects/2026-01-23-FO-04/trines-thoughts.md`
- FO-03 implementation: `app/fo-03/`
- Mastra agents: `src/mastra/agents/`
- Mastra documentation: `docs/current/mastra.md`
